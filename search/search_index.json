{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Steps for Object Detection on Raspberry-pi using Edge Impulse platform","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#install-raspberry-pi-os-on-raspberry-pi-4-model-b-using-raspberry-pi-imager","title":"Install Raspberry Pi OS on Raspberry Pi 4 model B using Raspberry Pi Imager","text":"<ul> <li>Download Raspberry Pi Imager from Rasberry Pi Imager for Windows to your system.</li> <li>Install it to SD card with SD card reader.</li> </ul>"},{"location":"#connect-raspberry-pi-to-your-internet-wifimobile-data","title":"Connect Raspberry Pi to your Internet (Wifi/Mobile Data)","text":"<ul> <li>You can use some IP scanner tool to scan and check if your Raspi is connected to your internet</li> <li>You can download Angry IP Scanner from here.</li> <li>Scan using IP scanner and identify IP address of RasPi</li> </ul>"},{"location":"#access-raspberry-pi-graphical-desktop-remotely","title":"Access Raspberry-pi Graphical Desktop remotely","text":"<ul> <li>To access graphical desktop of RasPi from your laptop, download vnc viewer from here.</li> <li>Add new connection in VNC viewer.</li> <li>Enter IP address of RasPi and password set for Raspi which was set when OS was installed on RasPi using Raspi Imager.</li> <li>Now RasPi Desktop will be visible on your laptop screen.</li> <li>Sometimes, it may not be visible in first attempt and can show following possible errors:</li> </ul>"},{"location":"#1-the-connection-was-refused-by-the-computer","title":"1. 'The connection was refused by the computer'","text":"<ul> <li>Open PUTTY</li> <li>Type IP address of Raspi in Host Name or IP address text box</li> <li>Enter username and password that you have set for Raspi while flashing OS on SD card</li> <li>Now you are logged in to raspi terminal</li> <li>Type command \u2018sudo raspi-config\u2019 on Raspi Terminal</li> <li>Go to \u2018Interface\u2019 option--&gt;&gt; vnc --&gt; enable --&gt;ok</li> <li>Now connect from vnc viewer again</li> </ul>"},{"location":"#2-timed-out-waiting-for-a-response-from-the-computer","title":"2. 'Timed out waiting for a response from the computer'","text":"<ul> <li>This issue can be resolved by configuring firewall to allow VNC Viewer port</li> <li>Search Windows Firewall in the search box and then select \"Windows Firewall with advanced security\" to open Port 5900.</li> <li>Select Inbound Rules from the left sidebar. Click New Rule in the right sidebar</li> <li>Select Port and click Next.</li> <li>Select TCP, enter 5900 in the Specific local port field, then click Next.</li> <li>Choose Allow the connection and click Next. Choose Domain and click Next. Enter a name for the rule, such as \u201cvnc\", and then click Finish.</li> <li>Restart the VNC Viewer</li> </ul>"},{"location":"#connecting-to-raspberry-pi-bullseye-camera","title":"Connecting to Raspberry Pi Bullseye Camera","text":"<ul> <li>Once you are able to access Raspi remotely from laptop, next step is to connect to Raspi camera</li> <li>Open Raspi terminal</li> <li>Type command <code>raspistill</code> on terminal</li> <li>Following possible error may occur:</li> </ul>"},{"location":"#1-can-not-currently-show-desktop","title":"1. 'Can not currently show Desktop'","text":"<ul> <li>To resolve this issue, connect to Raspi through ssh</li> <li>i.e. Open command prompt</li> <li>Type command <code>ssh@ip_address_of_Raspi</code></li> <li>Enter password set for Raspi while installing OS on it. </li> <li>At terminal type command <code>sudo nano  /boot/config.txt</code></li> <li>Config.txt file will be opened</li> <li>Do following modifications in <code>config.txt</code> file<ol> <li>Uncomment <code>hdmi_force_hotplug=1</code></li> <li>Uncomment `hdmi_group=1``</li> <li>Modify and uncomment <code>hdmi_model=1</code> to <code>hdmi_mode=16</code></li> <li>Under [pi4] title comment the first line.</li> <li>Save <code>config.txt</code> using <code>ctrl+s</code> and exit using <code>ctrl+x</code></li> </ol> </li> <li>Reboot the raspi using sudo reboot command</li> <li>Go to Raspi terminal and use command <code>raspistill</code></li> </ul>"},{"location":"#connecting-raspberry-pi-to-edge-impulse","title":"Connecting raspberry Pi to Edge Impulse","text":"<ul> <li>Install dependencies by running following commands</li> <li><code>sudo apt update</code></li> <li><code>curl -sL https://deb.nodesource.com/setup_12.x | sudo bash -</code></li> <li><code>sudo apt install -y gcc g++ make build-essential nodejs sox gstreamer1.0-tools gstreamer1.0-plugins-good gstreamer1.0-plugins-base gstreamer1.0-plugins-base-apps</code></li> <li><code>npm config set user root &amp;&amp; sudo npm install edge-impulse-linux -g --unsafe-perm</code></li> <li>Connect to edge impulse by running the command <code>edge-impulse-linux</code></li> <li>This will start a wizard which will ask you to log in, and choose an Edge Impulse project</li> <li>Raspberry Pi is now connected to Edge Impulse. To verify this, go to your Edge Impulse project, and   click Devices. The device will be listed here.</li> <li>Once the device is connected to Edge Impulse Platform, below sequence of steps are followed which includes steps from data collection to live inference.</li> </ul>"},{"location":"#1-data-acquisition","title":"1. Data Acquisition","text":"<ul> <li>Click on <code>Data acquisition</code> tab in edge impulse and capture images using Raspberry Pi camera</li> <li>Divide captured images into two sets. 80% images in training set and 20% images in test set</li> </ul>"},{"location":"#2-impulse-design-data-acquisition-data-processing-and-model-training","title":"2. Impulse Design (Data acquisition, Data processing and Model training)","text":"<ul> <li>Click on <code>Impulse design</code> tab. </li> <li>An impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.</li> <li>A complete Impulse will consist of 3 main building blocks: input block, processing block and a learning block.</li> <li>Click on <code>create impulse</code> option. Input block indicates type of input data you are training your model with. For Image input in our case, we use square image size. 360x360. Select this image size in input block.</li> <li>A processing block is basically a feature extractor. Select <code>pre-process and normalize images</code> option from available options.</li> <li>After adding processing block, next step is to add learning block. A learning block is simply a neural network that is trained to learn on your data. Learning blocks vary depending on what you want your model to do and the type of data in your training dataset. Select fine tuned pretrained model for object detection. It is recommended option by Edge Impulse.</li> <li>Save the created impulse.</li> <li>Click on <code>Image</code> tab under <code>Impulse Design</code> tab.</li> <li>Under <code>Parameters</code> tab click on <code>Save parameters</code></li> <li>Under <code>Generate Features</code> tab click on <code>Generate Features</code>. It will generate learned features.</li> <li>Now click on <code>Object Detection</code> tab under <code>Impulse Design</code> tab. Here you can customize the neural network model setting that you have selected in learning block. You can customize the parameters like <code>Number of training cycles</code>, <code>learning rate</code>, <code>validation set size</code> etc. </li> <li>Also, you can change the model if you want to and start training the model.</li> <li>Once the model is trained, it will give the performance of training model in the form of accuracy or precision score.</li> <li>It also displays <code>inference time</code> and <code>flash usage</code> for selected type of output device which is Raspberry Pi4 in this case</li> <li>You can fine tune neural network settings to get improved performance.</li> </ul>"},{"location":"#live-inference","title":"Live Inference","text":"<ul> <li>Once model is trained for its best performance, we can capture image from Raspberry Pi4 camera and classify it using our trained model i.e. live inference</li> <li>To get live inference by trained model, click on <code>Live Classification</code> model.</li> <li>Select <code>Device</code> as <code>Raspberry Pi</code>.</li> <li>Select <code>Sensor</code> as <code>camera</code> and click on <code>Start Sampling</code>.</li> <li>This will capture live image. Model immediately detects the object in current frame and labels it.</li> <li>It shows bounding box around the object with label and confidence level.</li> </ul>"},{"location":"#deploying-the-model-back-to-device","title":"Deploying the model back to device","text":"<ul> <li>Once the model is trained and tested for live inference, the trained model is deployed on the Raspberry Pi. It will run your impulse locally.</li> <li>Type command <code>edge-impulse-linux-runner</code> on Raspberry Pi terminal to run the impulse locally.</li> <li>This will automatically compile your model with full hardware acceleration, download the model to your Raspberry Pi, and then start object detection.</li> <li>It gives an URL on terminal. If you want to see the feed of camera and live object detection in browser, then open this url in browser and live object detection can be seen in browser.</li> </ul>"}]}